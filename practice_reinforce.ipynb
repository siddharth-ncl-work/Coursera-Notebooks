{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"practice_reinforce.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qpAz9pvmDxrF","colab_type":"text"},"source":["# REINFORCE in TensorFlow\n","\n","This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n","\n","It has been deliberately written to be as simple and human-readable.\n"]},{"cell_type":"markdown","metadata":{"id":"98QVEciLDxrK","colab_type":"text"},"source":["The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n","\n","In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"]},{"cell_type":"code","metadata":{"id":"RTCHDWC0DxrU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"41f607a5-9e75-4d93-fc0e-5a10a711341a","executionInfo":{"status":"ok","timestamp":1557346105802,"user_tz":-330,"elapsed":6406,"user":{"displayName":"Siddharth Ghule","photoUrl":"https://lh6.googleusercontent.com/-CRdt4DR4KLQ/AAAAAAAAAAI/AAAAAAAAM44/HF-YNi8zuAI/s64/photo.jpg","userId":"01016581137657955426"}}},"source":["#XVFB will be launched if you run on a server\n","import os\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n","    !bash ../xvfb start\n","    %env DISPLAY=:1"],"execution_count":1,"outputs":[{"output_type":"stream","text":["bash: ../xvfb: No such file or directory\n","env: DISPLAY=:1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R-jvKZCjDxsD","colab_type":"code","colab":{}},"source":["import gym\n","import numpy as np, pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","env = gym.make(\"CartPole-v0\")\n","\n","#gym compatibility: unwrap TimeLimit\n","if hasattr(env,'env'):\n","    env=env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","#plt.imshow(env.render(\"rgb_array\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f3q29JVfDxsl","colab_type":"text"},"source":["# Building the policy network"]},{"cell_type":"markdown","metadata":{"id":"YFGHMfgODxsp","colab_type":"text"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__. \n","\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"NfpakOyZDxst","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","tf.reset_default_graph()\n","#create input variables. We only need <s,a,R> for REINFORCE\n","states = tf.placeholder('float32',(None,)+state_dim,name=\"states\")\n","actions = tf.placeholder('int32',name=\"action_ids\")\n","cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nz4Oub_ZDxtG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"744ba016-e994-4243-8510-e579a081f426","executionInfo":{"status":"ok","timestamp":1557346128595,"user_tz":-330,"elapsed":903,"user":{"displayName":"Siddharth Ghule","photoUrl":"https://lh6.googleusercontent.com/-CRdt4DR4KLQ/AAAAAAAAAAI/AAAAAAAAM44/HF-YNi8zuAI/s64/photo.jpg","userId":"01016581137657955426"}}},"source":["tf.get_default_graph().get_operations()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Operation 'states' type=Placeholder>,\n"," <tf.Operation 'action_ids' type=Placeholder>,\n"," <tf.Operation 'cumulative_returns' type=Placeholder>]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"rJ78_e61Dxtf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"961befaf-8b2e-4a6a-e2f3-1edf4c73faa6","executionInfo":{"status":"ok","timestamp":1557346130702,"user_tz":-330,"elapsed":1329,"user":{"displayName":"Siddharth Ghule","photoUrl":"https://lh6.googleusercontent.com/-CRdt4DR4KLQ/AAAAAAAAAAI/AAAAAAAAM44/HF-YNi8zuAI/s64/photo.jpg","userId":"01016581137657955426"}}},"source":["\n","#<define network graph using raw tf or any deep learning library>\n","model=tf.keras.Sequential([\n","    tf.keras.layers.Dense(256,activation='relu'),\n","    tf.keras.layers.Dense(128,activation='relu'),\n","    tf.keras.layers.Dense(n_actions,activation='linear')\n","])\n","logits = model(states)#<linear outputs (symbolic) of your network>\n","\n","policy = tf.nn.softmax(logits)\n","log_policy = tf.nn.log_softmax(logits)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dugl2LDKDxt4","colab_type":"code","colab":{}},"source":["#tf.get_default_graph().get_operations()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yCcLj09cDxuO","colab_type":"code","colab":{}},"source":["#utility function to pick action in one given state\n","get_action_proba = lambda s: policy.eval({states:[s]})[0] "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P2Cfk_CADxui","colab_type":"text"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n","\n","\n","Following the REINFORCE algorithm, we can define our objective as follows: \n","\n","$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n","\n","When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"]},{"cell_type":"code","metadata":{"id":"zwOgws8iDxuq","colab_type":"code","colab":{},"outputId":"3a88767d-59bd-431a-d806-572c5409a170"},"source":["x = tf.constant([1, 4,10,1])\n","y = tf.constant([2, 5,10,1])\n","z = tf.constant([3, 6,10,1])\n","tf.stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)\n","tf.stack([x, y, z], axis=-1)  # [[1, 2, 3], [4, 5, 6]]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'stack_1:0' shape=(4, 3) dtype=int32>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"CnGZfHrbDxvF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"a2095f31-9db2-4303-a827-c19a69d22adf","executionInfo":{"status":"ok","timestamp":1557346136898,"user_tz":-330,"elapsed":1062,"user":{"displayName":"Siddharth Ghule","photoUrl":"https://lh6.googleusercontent.com/-CRdt4DR4KLQ/AAAAAAAAAAI/AAAAAAAAM44/HF-YNi8zuAI/s64/photo.jpg","userId":"01016581137657955426"}}},"source":["#get probabilities for parti\n","indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)\n","print(indices)\n","log_policy_for_actions = tf.gather_nd(log_policy,indices)\n","print(log_policy_for_actions)\n","indices = tf.stack([tf.range(tf.shape(policy)[0]),actions],axis=-1)\n","policy_for_actions=tf.gather_nd(policy,indices)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Tensor(\"stack:0\", shape=(?, 2), dtype=int32)\n","Tensor(\"GatherNd:0\", shape=(?,), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AQwX1RW4Dxvg","colab_type":"code","colab":{}},"source":["# policy objective as in the last formula. please use mean, not sum.\n","# note: you need to use log_policy_for_actions to get log probabilities for actions taken\n","#cum_reward_for_actions=tf.gather_nd(cumulative_rewards,indices)\n","J = tf.reduce_mean(log_policy_for_actions*cumulative_rewards)#<YOUR CODE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jo-vKibDDxvw","colab_type":"code","colab":{},"outputId":"9d71ecc7-2bd1-4a5c-dc15-631e810ee088"},"source":["print(cumulative_rewards.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<unknown>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T-zOqFjxDxwA","colab_type":"code","colab":{}},"source":["#regularize with entropy\n","entropy =0 - tf.reduce_mean(policy_for_actions*log_policy_for_actions)#<compute entropy. Don't forget the sign!>"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8HCCp4FuDxwP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"b3a9666d-739d-448a-8702-a557554c0192","executionInfo":{"status":"ok","timestamp":1557346144557,"user_tz":-330,"elapsed":1227,"user":{"displayName":"Siddharth Ghule","photoUrl":"https://lh6.googleusercontent.com/-CRdt4DR4KLQ/AAAAAAAAAAI/AAAAAAAAM44/HF-YNi8zuAI/s64/photo.jpg","userId":"01016581137657955426"}}},"source":["#all network weights\n","all_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)#<a list of all trainable weights in your network>\n","\n","#weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n","loss = -J -0.1 * entropy\n","\n","update = tf.train.AdamOptimizer().minimize(loss,var_list=all_weights)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gCiYhACDDxwg","colab_type":"text"},"source":["### Computing cumulative rewards"]},{"cell_type":"code","metadata":{"id":"1edF11PGDxwu","colab_type":"code","colab":{}},"source":["def get_cumulative_rewards(rewards, #rewards at each step\n","                           gamma = 0.99 #discount for reward\n","                           ):\n","    \"\"\"\n","    take a list of immediate rewards r(s,a) for the whole session \n","    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n","    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","    \n","    The simple way to compute cumulative rewards is to iterate from last to first time tick\n","    and compute R_t = r_t + gamma*R_{t+1} recurrently\n","    \n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    \n","    #<your code here>\n","    cum_reward=[]\n","    for j,r_t in enumerate(rewards):\n","        reward=0\n","        for i,r in enumerate(rewards[j:]):\n","            reward+=(gamma**i)*r\n","        cum_reward.append(reward)\n","    \n","    return cum_reward\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4pJ6elWADxxI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3700fb9c-a7c6-4ec4-dd79-737256f18a57","executionInfo":{"status":"ok","timestamp":1557346150982,"user_tz":-330,"elapsed":1311,"user":{"displayName":"Siddharth Ghule","photoUrl":"https://lh6.googleusercontent.com/-CRdt4DR4KLQ/AAAAAAAAAAI/AAAAAAAAM44/HF-YNi8zuAI/s64/photo.jpg","userId":"01016581137657955426"}}},"source":["assert len(get_cumulative_rewards(range(100))) == 100\n","assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ziMSAINBDxyH","colab_type":"code","colab":{}},"source":["def train_step(_states,_actions,_rewards):\n","    \"\"\"given full session, trains agent with policy gradient\"\"\"\n","    _cumulative_rewards = get_cumulative_rewards(_rewards)\n","    update.run({states:_states,actions:_actions,cumulative_rewards:_cumulative_rewards})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQZQtPUFDxyb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4OSSvexDDxyt","colab_type":"text"},"source":["### Playing the game"]},{"cell_type":"code","metadata":{"id":"lovnqHzJDxy3","colab_type":"code","colab":{}},"source":["def generate_session(t_max=1000):\n","    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n","    \n","    #arrays to record session\n","    states,actions,rewards = [],[],[]\n","    \n","    s = env.reset()\n","    \n","    for t in range(t_max):\n","        \n","        #action probabilities array aka pi(a|s)\n","        action_probas = get_action_proba(s)\n","        \n","        a = np.random.choice(range(n_actions),p=action_probas)#<pick random action using action_probas>\n","        \n","        new_s,r,done,info = env.step(a)\n","        \n","        #record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","        \n","        s = new_s\n","        if done: break\n","            \n","    train_step(states,actions,rewards)\n","            \n","    return sum(rewards)\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQTOAdAzDxzL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"b65d15a2-747b-44a2-ab45-2212880465e7","executionInfo":{"status":"ok","timestamp":1557346199879,"user_tz":-330,"elapsed":40652,"user":{"displayName":"Siddharth Ghule","photoUrl":"https://lh6.googleusercontent.com/-CRdt4DR4KLQ/AAAAAAAAAAI/AAAAAAAAM44/HF-YNi8zuAI/s64/photo.jpg","userId":"01016581137657955426"}}},"source":["s = tf.InteractiveSession()\n","s.run(tf.global_variables_initializer())\n","\n","for i in range(100):\n","    \n","    rewards = [generate_session() for _ in range(100)] #generate new sessions\n","    \n","    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n","\n","    if np.mean(rewards) > 300:\n","        print (\"You Win!\")\n","        break\n","        "],"execution_count":16,"outputs":[{"output_type":"stream","text":["mean reward:34.530\n","mean reward:92.420\n","mean reward:178.660\n","mean reward:371.000\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wf1qOu0CDxzY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5G8748gDxz8","colab_type":"code","colab":{},"outputId":"7e1f2764-462d-42b0-8907-39f093d6dc06"},"source":["s = tf.InteractiveSession()\n","s.run(tf.global_variables_initializer())\n","\n","for i in range(100):\n","    \n","    rewards = [generate_session() for _ in range(100)] #generate new sessions\n","    \n","    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n","\n","    if np.mean(rewards) > 300:\n","        print (\"You Win!\")\n","        break\n","        \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mean reward:27.590\n","mean reward:70.340\n","mean reward:129.570\n","mean reward:188.330\n","mean reward:211.530\n","mean reward:240.490\n","mean reward:235.760\n","mean reward:218.030\n","mean reward:258.470\n","mean reward:184.760\n","mean reward:298.920\n","mean reward:507.360\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZmUy3PwcDx0Q","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kheVVSnWDx0b","colab_type":"text"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"LwmppyyLDx0c","colab_type":"code","colab":{},"outputId":"8edd61b7-435a-4f08-a98a-80428c0375f7"},"source":["#record sessions\n","import gym.wrappers\n","env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n","sessions = [generate_session() for _ in range(100)]\n","env.close()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[2017-04-08 03:29:10,315] Making new env: CartPole-v0\n","[2017-04-08 03:29:10,324] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n","[2017-04-08 03:29:10,329] Clearing 6 monitor files from previous run (because force=True was provided)\n","[2017-04-08 03:29:10,336] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000000.mp4\n","[2017-04-08 03:29:16,834] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000001.mp4\n","[2017-04-08 03:29:23,689] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000008.mp4\n","[2017-04-08 03:29:33,407] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000027.mp4\n","[2017-04-08 03:29:45,840] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000064.mp4\n","[2017-04-08 03:29:56,812] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/jheuristic/Downloads/sonnet/sonnet/examples/videos')\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VUx-vzSVDx0t","colab_type":"code","colab":{},"outputId":"03634fb9-ea49-4115-ab2a-f0a76c688be7"},"source":["#show video\n","from IPython.display import HTML\n","import os\n","\n","video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"./videos/openaigym.video.0.14221.video000027.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"MK3wceGUEmPu","colab_type":"code","colab":{}},"source":["import re\n","import requests\n","import json\n","\n","\n","class Grader(object):\n","    def __init__(self, assignment_key, all_parts=()):\n","        \"\"\"\n","        Assignment key is the way to tell Coursera which problem is being submitted.\n","        \"\"\"\n","        self.submission_page = \\\n","            'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1'\n","        self.assignment_key = assignment_key\n","        self.answers = {part: None for part in all_parts}\n","\n","    def submit(self, email, token):\n","        submission = {\n","                    \"assignmentKey\": self.assignment_key,\n","                    \"submitterEmail\": email,\n","                    \"secret\": token,\n","                    \"parts\": {}\n","        }\n","        for part, output in self.answers.items():\n","            if output is not None:\n","                submission[\"parts\"][part] = {\"output\": output}\n","            else:\n","                submission[\"parts\"][part] = dict()\n","        request = requests.post(self.submission_page, data=json.dumps(submission))\n","        response = request.json()\n","        if request.status_code == 201:\n","            print('Submitted to Coursera platform. See results on assignment page!')\n","        elif u'details' in response and u'learnerMessage' in response[u'details']:\n","            print(response[u'details'][u'learnerMessage'])\n","        else:\n","            print(\"Unknown response from Coursera: {}\".format(request.status_code))\n","            print(response)\n","\n","    def set_answer(self, part, answer):\n","        \"\"\"Adds an answer for submission. Answer is expected either as string, number, or\n","           an iterable of numbers.\n","           Args:\n","              part - str, assignment part id\n","              answer - answer to submit. If non iterable, appends repr(answer). If string,\n","                is appended as provided. If an iterable and not string, converted to\n","                space-delimited repr() of members.\n","        \"\"\"\n","        if isinstance(answer, str):\n","            self.answers[part] = answer\n","        else:\n","            try:\n","                self.answers[part] = \" \".join(map(repr, answer))\n","            except TypeError:\n","                self.answers[part] = repr(answer)\n","\n","\n","def array_to_grader(array, epsilon=1e-4):\n","    \"\"\"Utility function to help preparing Coursera grading conditions descriptions.\n","    Args:\n","       array: iterable of numbers, the correct answers\n","       epslion: the generated expression will accept the answers with this absolute difference with\n","         provided values\n","    Returns:\n","       String. A Coursera grader expression that checks whether the user submission is in\n","         (array - epsilon, array + epsilon)\"\"\"\n","    res = []\n","    for element in array:\n","        if isinstance(element, int):\n","            res.append(\"[{0}, {0}]\".format(element))\n","        else:\n","            res.append(\"({0}, {1})\".format(element - epsilon, element + epsilon))\n","    return \" \".join(res)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ru0JuBTcDx0z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6ec77ded-36c5-480f-eb79-65f18e37eb66","executionInfo":{"status":"ok","timestamp":1557346367192,"user_tz":-330,"elapsed":57289,"user":{"displayName":"Siddharth Ghule","photoUrl":"https://lh6.googleusercontent.com/-CRdt4DR4KLQ/AAAAAAAAAAI/AAAAAAAAM44/HF-YNi8zuAI/s64/photo.jpg","userId":"01016581137657955426"}}},"source":["import sys\n","import numpy as np\n","sys.path.append(\"..\")\n","#import grading\n","\n","\n","def submit_cartpole(generate_session, email, token):\n","    sessions = [generate_session() for _ in range(100)]\n","    session_rewards = np.array(sessions)\n","    grader = Grader(\"oyT3Bt7yEeeQvhJmhysb5g\")\n","    grader.set_answer(\"7QKmA\", int(np.mean(session_rewards)))\n","    grader.submit(email, token)\n","\n","\n","submit_cartpole(generate_session, \"ss.ghule@ncl.res.in\", \"HM9E30DSYM5WoVaU\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Submitted to Coursera platform. See results on assignment page!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TGKedk60Dx0-","colab_type":"code","colab":{}},"source":["# That's all, thank you for your attention!\n","# Not having enough? There's an actor-critic waiting for you in the honor section.\n","# But make sure you've seen the videos first."],"execution_count":0,"outputs":[]}]}